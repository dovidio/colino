"""
Digest Manager - Handles all digest operations for Colino
"""

import logging
import os
from datetime import UTC, datetime, timedelta
from typing import Any

from .config import config
from .db import Database
from .ingest_manager import IngestManager
from .sources.rss import RSSSource
from .sources.youtube import YouTubeSource
from .summarize import DigestGenerator

logger = logging.getLogger(__name__)


class DigestManager:
    """Manages all digest operations including URL-based, post-based, and recent articles"""

    def __init__(self, db: Database | None = None):
        self.db = db or Database()
        self.digest_generator = DigestGenerator()

    def digest_url(self, url: str, output_file: str | None = None) -> bool:
        """
        Digest content from a specific URL

        Args:
            url: The URL to digest (YouTube video or website)
            output_file: Optional path to save digest to file

        Returns:
            bool: True if successful, False otherwise
        """
        logger.info(f"Processing digest for URL: {url}")

        try:
            # First, try to find content by ID (the URL itself might be the ID)
            existing_content = self.db.get_content_by(url)

            # If not found by ID, try to find by URL
            if not existing_content:
                existing_content = self.db.get_content_by_url(url)

            if existing_content:
                return self._digest_cached_content(existing_content, output_file)
            else:
                return self._digest_fresh_url(url, output_file)

        except ValueError as e:
            self._handle_api_error(e)
            return False
        except Exception as e:
            logger.error(f"Error processing URL digest: {e}")
            print(f"‚ùå Error processing URL: {e}")
            return False

    def digest_recent_articles(
        self,
        hours: int | None = None,
        output_file: str | None = None,
        source: str | None = None,
        auto_ingest: bool = True,
    ) -> bool:
        """
        Generate digest of recent articles

        Args:
            hours: Hours to look back (uses config default if None)
            output_file: Optional path to save digest to file
            source: Filter by source ('rss' or 'youtube')
            auto_ingest: Whether to automatically ingest recent content before digesting

        Returns:
            bool: True if successful, False otherwise
        """
        hours = hours or config.DEFAULT_LOOKBACK_HOURS
        since_time = datetime.now(UTC) - timedelta(hours=hours)

        # Auto-ingest recent content if enabled
        if auto_ingest:
            self._auto_ingest_recent_content(source, hours)

        source_filter = f" from {source}" if source else ""
        logger.info(
            f"Generating digest for articles{source_filter} from last {hours} hours"
        )

        try:
            # Get recent posts from database
            posts = self.db.get_content_since(since_time, source=source)

            if not posts:
                print(f"‚ùå No posts found{source_filter} from the last {hours} hours")
                print("   Try running 'colino ingest' first or increase --hours")
                return False

            print(
                f"ü§ñ Generating AI digest for {len(posts)} recent articles{source_filter}..."
            )
            print(f"   Using model: {config.LLM_MODEL}")

            # Generate digest
            digest_content = self.digest_generator.summarize_articles(posts)

            # Auto-save if enabled or output_file specified
            if config.AI_AUTO_SAVE:
                if not output_file:
                    # Auto-generate filename
                    os.makedirs(config.AI_SAVE_DIRECTORY, exist_ok=True)
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    source_suffix = f"_{source}" if source else ""
                    output_file = f"{config.AI_SAVE_DIRECTORY}/digest{source_suffix}_{timestamp}.md"

                with open(output_file, "w", encoding="utf-8") as f:
                    f.write(digest_content)
                print(f"‚úÖ Digest saved to {output_file}")

            # Always show digest in console unless explicitly saving to file
            if not output_file or config.AI_AUTO_SAVE:
                print("\n" + "=" * 60)
                print(digest_content)
                print("=" * 60)

            return True

        except ValueError as e:
            self._handle_api_error(e)
            return False
        except Exception as e:
            logger.error(f"Error generating digest: {e}")
            print(f"‚ùå Error generating digest: {e}")
            return False

    def _digest_cached_content(
        self, existing_content: dict[str, Any], output_file: str | None = None
    ) -> bool:
        """Digest content that was found in cache"""
        print(
            f"‚úÖ Found cached content for {existing_content.get('url', 'unknown URL')}"
        )
        print(f"   Source: {existing_content['source']}")
        print(f"   Cached at: {existing_content['fetched_at']}")

        # Generate digest from cached content
        digest_content = self.digest_generator.summarize_article(existing_content)

        # Save or display digest
        self._save_or_display_digest(
            digest_content, output_file, f"cached_{existing_content['source']}"
        )
        return True

    def _digest_fresh_url(self, url: str, output_file: str | None = None) -> bool:
        """Digest content from a fresh URL (not in cache)"""
        print(f"üîç Content not found in cache for {url}")

        # Check if it's a YouTube URL
        youtube_source = YouTubeSource()
        video_id = youtube_source.extract_video_id(url)

        if video_id:
            return self._digest_fresh_youtube(video_id, output_file)
        else:
            return self._digest_fresh_website(url, output_file)

    def _digest_fresh_youtube(
        self, video_id: str, output_file: str | None = None
    ) -> bool:
        """Digest a fresh YouTube video"""
        print(f"üì∫ Detected YouTube video: {video_id}")
        print("üé¨ Fetching transcript...")

        youtube_source = YouTubeSource()
        transcript = youtube_source.get_video_transcript(video_id)

        if not transcript:
            print("‚ùå No transcript available for this video")
            return False

        print(f"‚úÖ Extracted transcript ({len(transcript)} characters)")

        # Generate digest from transcript
        digest_content = self.digest_generator.summarize_video(transcript)

        # Save or display digest
        self._save_or_display_digest(digest_content, output_file, "youtube_video")
        return True

    def _digest_fresh_website(self, url: str, output_file: str | None = None) -> bool:
        """Digest a fresh website URL"""
        print("üåê Detected website URL")
        print("üìÑ Fetching and processing content...")

        # For regular websites, use RSS scraper to get content
        rss_source = RSSSource()
        scraped_content = rss_source.scraper.scrape_article_content(url)

        if not scraped_content:
            print("‚ùå Could not extract content from the website")
            return False

        print(f"‚úÖ Extracted content ({len(scraped_content)} characters)")

        # Create article data structure for digest
        article_data = {
            "title": "Scraped Article",
            "feed_title": "",
            "content": scraped_content,
            "url": url,
            "source": "website",
            "published": datetime.now().isoformat(),
        }

        # Generate digest
        digest_content = self.digest_generator.generate_llm_article_digest(article_data)

        # Save or display digest
        self._save_or_display_digest(digest_content, output_file, "website")
        return True

    def _save_or_display_digest(
        self, digest_content: str, output_file: str | None = None, source_type: str = ""
    ):
        """Helper function to save or display digest content"""
        # Auto-save if enabled or output_file specified
        if output_file or config.AI_AUTO_SAVE:
            if not output_file:
                # Auto-generate filename
                os.makedirs(config.AI_SAVE_DIRECTORY, exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                source_suffix = f"_{source_type}" if source_type else ""
                output_file = (
                    f"{config.AI_SAVE_DIRECTORY}/digest{source_suffix}_{timestamp}.md"
                )

            with open(output_file, "w", encoding="utf-8") as f:
                f.write(digest_content)
            print(f"‚úÖ Digest saved to {output_file}")

        # Always show digest in console unless explicitly saving to file
        if not output_file or config.AI_AUTO_SAVE:
            print("\n" + "=" * 60)
            print(digest_content)
            print("=" * 60)

    def _handle_api_error(self, e: ValueError):
        """Handle API-related errors"""
        if "openai_api_key" in str(e) or "Incorrect API key" in str(e):
            print("‚ùå OpenAI API key not configured or invalid")
            print("   Set environment variable: export OPENAI_API_KEY='your_key_here'")
            print("   Get one from: https://platform.openai.com/api-keys")
        else:
            print(f"‚ùå Configuration error: {e}")

    def _auto_ingest_recent_content(
        self, source: str | None = None, hours: int | None = None
    ):
        """
        Automatically ingest recent content before digesting

        Args:
            source: Source to ingest from ('rss', 'youtube', or None for all)
            hours: Hours to look back for ingestion
        """
        print("üîÑ Auto-ingesting recent content before generating digest...")

        # Determine which sources to ingest
        if source:
            sources = [source]
        else:
            sources = ["rss", "youtube"]

        # Use IngestManager to ingest recent content
        ingest_manager = IngestManager(db=self.db)
        ingested_posts = ingest_manager.ingest(sources, hours)

        if ingested_posts:
            print(f"‚úÖ Auto-ingested {len(ingested_posts)} recent posts")
        else:
            print("üìã No new posts to ingest")
